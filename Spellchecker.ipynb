{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kpu/kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* языковая модель\n",
    "* лексикон\n",
    "* модель ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import os\n",
    "import re\n",
    "import fuzzyset\n",
    "import nltk\n",
    "from spell_checker import find_spell_errors\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "latin_model = kenlm.Model('../../kenlm/lm/latin_3gram.arpa')\n",
    "sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenize = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_list(path_ref, path_error, even):\n",
    "    \"\"\"\n",
    "    return list (word, word with error)\n",
    "    \"\"\"\n",
    "    lat_sent = fuzzyset.FuzzySet()\n",
    "    for book in os.listdir(path_ref):\n",
    "        with open(path_ref + book, \"r\") as f:\n",
    "            for s in sent_tokenize.tokenize(f.read().lower()):\n",
    "                lat_sent.add(s)\n",
    "    \n",
    "    text = \"\"\n",
    "    for pages in sorted(os.listdir(path_error), key = lambda x: int(x[:-4])):\n",
    "        if even:\n",
    "            if int(pages[:-4]) % 2 == 0:\n",
    "                with open(path_error + pages, 'r') as f:\n",
    "                    text += re.sub(\"\\n\", \" \", re.sub(\"-\\n\", \"\", f.read().lower()))\n",
    "        else:\n",
    "            if int(pages[:-4]) % 2 != 0:\n",
    "                with open(path_error + pages, 'r') as f:\n",
    "                    text += re.sub(\"\\n\", \" \", re.sub(\"-\\n\", \"\", f.read().lower()))\n",
    "    latin_OCR = sent_tokenize.tokenize(text)\n",
    "    print(\"text error done\", len(latin_OCR))\n",
    "    \n",
    "    align_sent = []\n",
    "    for lat_ocr in latin_OCR:\n",
    "        try:\n",
    "            if lat_sent.get(lat_ocr)[0][0] < 1 and lat_sent.get(lat_ocr)[0][0] > 0.7:\n",
    "                align_sent.append((lat_ocr, lat_sent.get(lat_ocr)[0][1]))\n",
    "        except:\n",
    "            pass\n",
    "    print(\"align sent done\", len(align_sent))\n",
    "    \n",
    "    words = []\n",
    "    for s_ocr, s_ref in align_sent:\n",
    "        f_ref = fuzzyset.FuzzySet(word_tokenize.tokenize(s_ref))\n",
    "        for w in word_tokenize.tokenize(s_ocr):\n",
    "            if w not in \".';/,-?!\":\n",
    "                try:\n",
    "                    if f_ref.get(w)[0][0] > 0.7 and f_ref.get(w)[0][1] != w:\n",
    "                        words.append((f_ref.get(w)[0][1], w))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    print(\"words done\", len(words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text error done 2867\n",
      "align sent done 1333\n",
      "words done\n"
     ]
    }
   ],
   "source": [
    "words_1 = words_list(\"./spell_checker_data/edit_text/GaiusJuliusCaesar/\", \"./data_text_new/L072/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text error done 3989\n",
      "align sent done 1876\n",
      "words done\n"
     ]
    }
   ],
   "source": [
    "words_2 = words_list(\"./spell_checker_data/edit_text/lucan/\", \"./data_text_new/L220/\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Words \", len(words_1) + len(words_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon done\n"
     ]
    }
   ],
   "source": [
    "def make_lexicon(text):\n",
    "    lexicon = list(set(text))\n",
    "    with open(\"./spell_checker_data/noisy_channel_lexicon.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(lexicon))\n",
    "    print(\"lexicon done\")\n",
    "\n",
    "with open(\"./corpora_lat.txt\", \"r\") as f:\n",
    "    corpora_lat = f.read().lower()\n",
    "\n",
    "corpora_lat = word_tokenize.tokenize(corpora_lat)\n",
    "make_lexicon(corpora_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin words in corpora 525249\n"
     ]
    }
   ],
   "source": [
    "print(\"latin words in corpora\", len(set(corpora_lat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525249"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./spell_checker_data/noisy_channel_lexicon.txt\", \"r\") as f:\n",
    "     lexicon = f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_model(words):\n",
    "    substition = {}; insertion = {}; deletion = {}\n",
    "    for w_cor, w_error in words:\n",
    "        sub, ins, dl = find_spell_errors(w_cor, w_error)\n",
    "        for s in sub.keys():\n",
    "            if s not in substition.keys():\n",
    "                substition[s] = sub[s]\n",
    "            else:\n",
    "                substition[s] += sub[s]\n",
    "\n",
    "        for i in ins.keys():\n",
    "            if i not in insertion.keys():\n",
    "                insertion[i] = ins[i]\n",
    "            else:\n",
    "                insertion[i] += ins[i]\n",
    "\n",
    "        for d in dl.keys():\n",
    "            if d not in deletion.keys():\n",
    "                deletion[d] = dl[d]\n",
    "            else:\n",
    "                deletion[d] += dl[d]\n",
    "    \n",
    "    with open(\"./spell_checker_data/substitions.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join([s[0] + \" \" + s[1] + \" \" + str(self.substition[s]) for s in self.substition.keys()]))\n",
    "\n",
    "    with open(\"./spell_checker_data/insertion.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join([s[0] + \" \" + s[1] + \" \" + str(self.insertion[s]) for s in self.insertion.keys()]))\n",
    "\n",
    "    with open(\"./spell_checker_data/deletion.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join([s[0] + \" \" + s[1] + \" \" + str(self.deletion[s]) for s in self.deletion.keys()]))\n",
    "    \n",
    "    print(\"confusion matrix done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* заменить на additive smoothing (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybktree\n",
    "import editdistance\n",
    "import operator\n",
    "from math import log\n",
    "\n",
    "class noisy_channel:\n",
    "    def __init__(self, lexicon):\n",
    "        self.lexicon_tree = pybktree.BKTree(editdistance.eval, lexicon)\n",
    "        print(\"lexicon_tree done\")\n",
    "            \n",
    "        with open(\"./spell_checker_data/substitions.txt\", \"r\") as f:\n",
    "                self.substition = {(i.split()[0], i.split()[1]): int(i.split()[2]) for i in f.read().split(\"\\n\")}\n",
    "            \n",
    "        with open(\"./spell_checker_data/insertion.txt\", \"r\") as f:\n",
    "            self.insertion = {(i.split()[0], i.split()[1]): int(i.split()[2]) for i in f.read().split(\"\\n\")}\n",
    "            \n",
    "        with open(\"./spell_checker_data/deletion.txt\", \"r\") as f:\n",
    "            self.deletion = {(i.split()[0], i.split()[1]): int(i.split()[2]) for i in f.read().split(\"\\n\")}\n",
    "            \n",
    "        n_sub = sum(self.substition.values())\n",
    "        n_ins = sum(self.insertion.values())\n",
    "        n_del = sum(self.deletion.values())\n",
    "            \n",
    "        self.substition = {s: -log(self.substition[s] / (n_sub + n_ins + n_del), 10) for s in self.substition.keys()}\n",
    "        self.insertion = {s: -log(self.insertion[s] / (n_sub + n_ins + n_del), 10) for s in self.insertion.keys()}\n",
    "        self.deletion = {s: -log(self.deletion[s] / (n_sub + n_ins + n_del), 10) for s in self.deletion.keys()}\n",
    "\n",
    "        self.sub_none = max(self.substition.values())\n",
    "        self.ins_none = max(self.insertion.values())\n",
    "        self.del_none = max(self.deletion.values())\n",
    "        print(\"confusion matrix done\")\n",
    "        \n",
    "    def predict_score(self, word, contex, lang_model):\n",
    "        candidats = self.lexicon_tree.find(word, 2)\n",
    "        candidats_weighted = {}\n",
    "        for candidat in candidats:\n",
    "            sub, ins, dl = find_spell_errors(candidat[1], word)\n",
    "            sub_sum = sum([self.substition[s] * candidat[0] if s in self.substition.keys() else self.sub_none * candidat[0] for s in sub.keys()])\n",
    "            ins_sum = sum([self.insertion[s] * candidat[0]  if s in self.insertion.keys() else self.ins_none * candidat[0] for s in ins.keys()])\n",
    "            del_sum = sum([self.deletion[s] * candidat[0] if s in self.deletion.keys() else self.del_none * candidat[0] for s in dl.keys()])\n",
    "            levenshtein_weighted = sub_sum + ins_sum + del_sum\n",
    "            сond_prob = lang_model.score(contex + \" \" + candidat[1]) - lang_model.score(contex) #logP(c|a,b) = logP(a,b,c)-logP(a,b). kenLM\n",
    "            candidats_weighted[candidat] = levenshtein_weighted - сond_prob\n",
    "        \n",
    "        if candidats_weighted:\n",
    "            return min(candidats_weighted.items(), key = lambda kv: kv[1])\n",
    "        return None\n",
    "    \n",
    "    def fix_fragment(self, sentence, lang_model):\n",
    "        tokenize = word_tokenize.tokenize(sentence)\n",
    "        correct_sentence = []\n",
    "        w1 = self.predict_score(tokenize[0], \"\", lang_model)\n",
    "        if w1:\n",
    "            correct_sentence.append(w1[0][1])\n",
    "        else:\n",
    "            correct_sentence.append(tokenize[0])\n",
    "        \n",
    "        w2 = self.predict_score(tokenize[1], tokenize[0], lang_model)\n",
    "        if w2:\n",
    "            correct_sentence.append(w2[0][1])\n",
    "        else:\n",
    "            correct_sentence.append(tokenize[1])\n",
    "        \n",
    "        for w1, w2, w3 in nltk.trigrams(tokenize):\n",
    "            smth = self.predict_score(w3, w1 + \" \" + w2, lang_model)\n",
    "            if smth:\n",
    "                correct_sentence.append(smth[0][1])\n",
    "            else:\n",
    "                correct_sentence.append(w3)\n",
    "        return ' '.join(correct_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon_tree done\n",
      "confusion matrix done\n"
     ]
    }
   ],
   "source": [
    "model = noisy_channel(lexicon, load = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jamspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jamspell\n",
    "from cer_and_wer import CER, WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_corrector = jamspell.TSpellCorrector()\n",
    "latin_corrector.LoadLangModel(\"./model_lat.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def test_spellchecker(path_ref, path_error):\n",
    "    text_ref = \"\"\n",
    "    for book in sorted(os.listdir(path_ref)):\n",
    "        with open(path_ref + book, \"r\") as f:\n",
    "            text_ref += re.sub(\"\\n\", \"\", f.read().lower())\n",
    "    \n",
    "    text_OCR = \"\"\n",
    "    for pages in sorted(os.listdir(path_error), key = lambda x: int(x[:-4])):\n",
    "        if int(pages[:-4]) % 2 == 0:\n",
    "            with open(path_error + pages, 'r') as f:\n",
    "                text_OCR += re.sub(\"\\n\", \" \", re.sub(\"-\\n\", \"\", f.read().lower()))\n",
    "    \n",
    "    return text_ref, text_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ref, text_OCR = test_spellchecker(\"./spell_checker_data/edit_text/GaiusJuliusCaesar/\", \"./data_text_new/L072/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latin done\n",
      "english done\n"
     ]
    }
   ],
   "source": [
    "import lang_detector\n",
    "detector = lang_detector.language_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_one_book(path, source, name_book, latin_corrector, detector):\n",
    "    \"\"\"\n",
    "    fix one book with Jamspell\n",
    "    \"\"\"\n",
    "    os.mkdir(path + name_book + '/')\n",
    "    os.mkdir(path + name_book + '/' + \"eng/\")\n",
    "    os.mkdir(path + name_book + '/' + \"lat/\")\n",
    "    \n",
    "    all_pages = os.listdir(source)\n",
    "    all_pages.sort(key = lambda s1: int(s1[:-4]))\n",
    "    \n",
    "    with open(source + all_pages[69], \"r\") as f:\n",
    "        d = detector.predict(re.sub(\"\\n\", \" \", re.sub(\"-\\n\", \"\", f.read().lower())))\n",
    "    \n",
    "    for page in all_pages:\n",
    "        with open(source + page, \"r\") as r_file:\n",
    "            text = re.sub(\"\\n\", \" \", re.sub(\"-\\n\", \"\", r_file.read().lower()))\n",
    "            if d:\n",
    "                if int(page[:-4]) % 2 == 0:\n",
    "                    with open(path + name_book + '/' + \"lat/\" + page, \"w\") as w_file:\n",
    "                        w_file.write(' '.join([latin_corrector.FixFragment(s) for s in sent_tokenize.tokenize(text)]))\n",
    "                else:\n",
    "                    with open(path + name_book + '/' + \"eng/\" + page, \"w\") as w_file:\n",
    "                        w_file.write(text)\n",
    "            else:\n",
    "                if int(page[:-4]) % 2 == 0:\n",
    "                    with open(path + name_book + '/' + \"eng/\" + page, \"w\") as w_file:\n",
    "                        w_file.write(text)\n",
    "                else:\n",
    "                    with open(path + name_book + '/' + \"lat/\" + page, \"w\") as w_file:\n",
    "                        w_file.write(' '.join([latin_corrector.FixFragment(s) for s in sent_tokenize.tokenize(text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 22s, sys: 6.13 s, total: 22min 28s\n",
      "Wall time: 24min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "edit_test_2 = ' '.join(word_tokenize.tokenize(text_OCR)[:5000])\n",
    "edit_test_2 = [model.fix_fragment(s, latin_model) for s in sent_tokenize.tokenize(edit_test_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER OCR (Jamspell) vs ref ([195, 192, 193], 0.1162)\n"
     ]
    }
   ],
   "source": [
    "edit_test_1 = [latin_corrector.FixFragment(s) for s in sent_tokenize.tokenize(text_OCR)]\n",
    "edit_test_1 = word_tokenize.tokenize(\" \".join(edit_test_1))\n",
    "print(\"WER OCR (Jamspell) vs ref\", WER(edit_test_1[:5000], word_tokenize.tokenize(text_ref)[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER OCR (noisy_model) vs ref ([170, 192, 193], 0.1112)\n"
     ]
    }
   ],
   "source": [
    "edit_test_2 = ' '.join(word_tokenize.tokenize(text_OCR)[:5000])\n",
    "edit_test_2 = [model.fix_fragment(s, latin_model) for s in sent_tokenize.tokenize(edit_test_2)]\n",
    "edit_test_2 = word_tokenize.tokenize(\" \".join(edit_test_2))\n",
    "print(\"WER OCR (noisy_model) vs ref\", WER(edit_test_2, word_tokenize.tokenize(text_ref)[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, corpora):\n",
    "        OOV_approximation = 0\n",
    "        \n",
    "        self.tri_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.bi_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.un_model = defaultdict(lambda: OOV_approximation)\n",
    "        \n",
    "        for s in corpora:\n",
    "            for w1, w2, w3 in trigrams(s.split()):\n",
    "                self.tri_model[(w1, w2)][w3] += 1\n",
    "            for w1, w2 in bigrams(s.split()):\n",
    "                self.bi_model[w1][w2] += 1\n",
    "            for w1 in s.split():\n",
    "                self.un_model[w1] += 1\n",
    "        \n",
    "        for pair in self.tri_model: # p(w_i | w_i-1, w_i-2)\n",
    "            total_count = float(sum(self.tri_model[pair].values()))\n",
    "            for w3 in self.tri_model[pair]:\n",
    "                self.tri_model[pair][w3] /= total_count\n",
    "        \n",
    "        alpha_1 = 1\n",
    "        \n",
    "        for w2 in self.bi_model:\n",
    "            total_count = float(sum(self.bi_model[w2].values())) * alpha_1\n",
    "            for w1 in self.bi_model[w2]:\n",
    "                self.bi_model[w2][w1] /= total_count\n",
    "        \n",
    "        alpha_2 = 1\n",
    "        for w1 in self.un_model:\n",
    "            self.un_model[w1] = float(self.un_model[w1] * alpha_1 * alpha_2) / len(self.un_model)\n",
    "    \n",
    "    def predict_score(self, sentence):\n",
    "        s = sentence.split()\n",
    "        n = float(len(s))\n",
    "        \n",
    "        final_sum = 0\n",
    "        for i in range(2, int(n)):\n",
    "            if self.tri_model[(s[i - 2], s[i - 1])][s[i]] != 0:\n",
    "                final_sum += math.log(self.tri_model[(s[i - 2], s[i - 1])][s[i]], 2)\n",
    "            elif self.bi_model[s[i - 1]][s[i]] != 0:\n",
    "                final_sum += math.log(self.bi_model[s[i - 1]][s[i]], 2)\n",
    "            else:\n",
    "                final_sum += math.log(self.un_model[s[i]], 2)\n",
    "            \n",
    "        return (1 / n) * (final_sum + self.bi_model[s[1]][s[0]] + self.un_model[s[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
